{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Augmented AI (Amazon A2I) integration with Amazon SageMaker Hosted Endpoint for Object Detection and Model Retraining [Example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "4. [Create Control Plane Resources](#Create-Control-Plane-Resources)\n",
    "    1. [Create Human Task UI](#Create-Human-Task-UI)\n",
    "    2. [Create the Flow Definition](#Create-the-Flow-Definition)\n",
    "5. [Starting Human Loops](#Starting-Human-Loops)\n",
    "    1. [Wait For Workers to Complete Task](#Wait-For-Workers-to-Complete-Task)\n",
    "    2. [Check Status of Human Loop](#Check-Status-of-Human-Loop)\n",
    "    3. [View Task Results](#View-Task-Results)\n",
    "6. [Incremental training with SageMaker (Optional)](#Incremental-training-with-SageMaker-(Optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon Augmented AI (Amazon A2I) makes it easy to build the workflows required for human review of ML predictions. Amazon A2I brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers. \n",
    "\n",
    "You can create your own workflows for ML models built on Amazon SageMaker or any other tools. Using Amazon A2I, you can allow human reviewers to step in when a model is unable to make a high confidence prediction or to audit its predictions on an on-going basis. \n",
    "\n",
    "Learn more here: https://aws.amazon.com/augmented-ai/\n",
    "\n",
    "In this tutorial, we will show how you can use **Amazon A2I with an Amazon SageMaker Hosted Endpoint.** We will be using an exisiting object detection endpoint in this notebook. We will also demonstrate how to manipulate the A2I output to perform incremental training to improve the model accuracy with the newly labeled data using A2I.\n",
    "\n",
    "For more in depth instructions, visit https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate Amazon A2I into your human review workflows, you need three resources:\n",
    "\n",
    "* A **worker task template** to create a worker UI. The worker UI displays your input data, such as documents or images, and instructions to workers. It also provides interactive tools that the worker uses to complete your tasks. For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-instructions-overview.html\n",
    "\n",
    "* A **human review workflow**, also referred to as a flow definition. You use the flow definition to configure your human workforce and provide information about how to accomplish the human review task. You can create a flow definition in the Amazon Augmented AI console or with Amazon A2I APIs. To learn more about both of these options, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html\n",
    "\n",
    "* A **human loop** to start your human review workflow. When you use one of the built-in task types, the corresponding AWS service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met or for each object if no conditions were specified. When a human loop is triggered, human review tasks are sent to the workers as specified in the flow definition.\n",
    "\n",
    "When using a custom task type, as this tutorial will show, you start a human loop using the Amazon Augmented AI Runtime API. When you call `start_human_loop()` in your custom application, a task is sent to human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This notebook is developed and tested in a SageMaker Notebook Instance with a `ml.t2.medium` instance with SageMaker Python SDK v2. It is recommended to execute the notebook in the same environment for best experience.\n",
    "### Install Latest SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "assert parse_version(sagemaker.__version__) >= parse_version('2'), \\\n",
    "    '''This notebook is only compatible with sagemaker python SDK >= 2. \n",
    "Current version is %s. Please make sure you upgrade the library.''' % sagemaker.__version__\n",
    "\n",
    "print('SageMaker python SDK version: %s' % sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up the following data:\n",
    "* `region` - Region to call A2I.\n",
    "* `BUCKET` - A S3 bucket accessible by the given role\n",
    "    * Used to store the sample images & output results\n",
    "    * Must be within the same region A2I is called from\n",
    "* `role` - The IAM role used as part of StartHumanLoop. By default, this notebook will use the execution role\n",
    "* `workteam` - Group of people to send the work to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region \n",
    "region = '<REGION>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role and Permissions\n",
    "\n",
    "The AWS IAM Role used to execute the notebook needs to have the following permissions:\n",
    "\n",
    "* SagemakerFullAccess\n",
    "* AmazonSageMakerMechanicalTurkAccess (if using MechanicalTurk as your Workforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "role = get_execution_role()\n",
    "display(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Bucket and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = sess.default_bucket()\n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'\n",
    "MODEL_PATH = f's3://{BUCKET}/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection with Amazon SageMaker\n",
    "\n",
    "To demonstrate A2I with Amazon SageMaker hosted endpoint, we will take a trained object detection model from a S3 bucket and host it on the SageMaker endpoint for real-time prediction. Wonder how the model is trained? Training a computer vision model with SageMaker is easy, please follow [this notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) to train a object detection model using Single Shot multibox Detector (SSD) algorithm and PASCAL VOC dataset and host it for real-time prediction. The provided model is trained following [the notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model and create an endpoint\n",
    "The next cell will setup an endpoint from a trained model. It will take about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "source_model_data_s3_uri = 's3://aws-sagemaker-augmented-ai-example/model/model.tar.gz'\n",
    "\n",
    "!aws s3 cp {source_model_data_s3_uri} {MODEL_PATH}/model.tar.gz\n",
    "\n",
    "model_data_s3_uri = f'{MODEL_PATH}/model.tar.gz'\n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_name = 'DEMO-object-detection-augmented-ai-' + timestamp\n",
    "\n",
    "image = sagemaker.image_uris.retrieve('object-detection', region, version='1')\n",
    "model = sagemaker.model.Model(image, \n",
    "                              model_data = model_data_s3_uri,\n",
    "                              role = role,\n",
    "                              predictor_cls = sagemaker.predictor.Predictor,\n",
    "                              sagemaker_session = sess)\n",
    "\n",
    "object_detector = model.deploy(initial_instance_count = 1,\n",
    "                               instance_type = 'ml.m4.xlarge',\n",
    "                               endpoint_name = endpoint_name,\n",
    "                               serializer = sagemaker.serializers.IdentitySerializer('image/jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### object detection helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches    \n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "    \"\"\"\n",
    "    visualize detections in one image\n",
    "    Parameters:\n",
    "    ----------\n",
    "    img : numpy.array\n",
    "        image, in bgr format\n",
    "    dets : numpy.array\n",
    "        ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "        each row is one object\n",
    "    classes : tuple or list of str\n",
    "        class names\n",
    "    thresh : float\n",
    "        score threshold\n",
    "    \"\"\"\n",
    "    img=mpimg.imread(img_file)\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    colors = dict()\n",
    "    output = []\n",
    "    for det in dets:\n",
    "        (klass, score, x0, y0, x1, y1) = det\n",
    "        cls_id = int(klass)\n",
    "        class_name = str(cls_id)\n",
    "        if classes and len(classes) > cls_id:\n",
    "            class_name = classes[cls_id]\n",
    "        output.append([class_name, score])\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        if cls_id not in colors:\n",
    "            colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "        xmin = int(x0 * width)\n",
    "        ymin = int(y0 * height)\n",
    "        xmax = int(x1 * width)\n",
    "        ymax = int(y1 * height)\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "        ax.text(xmin, ymin - 2,\n",
    "                '{:s} {:.3f}'.format(class_name, score),\n",
    "                bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                          fontsize=12, color='white')\n",
    "\n",
    "    return f, output\n",
    "    \n",
    "def load_and_predict(file_name, predictor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    load an image, make object detection to an predictor, and visualize detections\n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_name : str\n",
    "        image file location, in str format\n",
    "    predictor : sagemaker.predictor.RealTimePredictor\n",
    "        a predictor loaded from hosted endpoint\n",
    "    threshold : float\n",
    "        score threshold for bounding box display\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as image:\n",
    "        f = image.read()\n",
    "        b = bytearray(f)\n",
    "    results = predictor.predict(b)\n",
    "    detections = json.loads(results)\n",
    "    \n",
    "    fig, detection_filtered = visualize_detection(file_name, detections['prediction'], \n",
    "                                                   object_categories, threshold)\n",
    "    return results, detection_filtered, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "                     'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "                     'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data\n",
    "Let's take a look how the object detection looks like using some stock photos on the internet. The predicted class and the prediction probability is visualized along with the bounding box using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_photos_index = ['980382', '276517', '1571457']\n",
    "\n",
    "if not os.path.isdir('sample-a2i-images'):\n",
    "    os.mkdir('sample-a2i-images')\n",
    "    \n",
    "for ind in test_photos_index:\n",
    "    !curl https://images.pexels.com/photos/{ind}/pexels-photo-{ind}.jpeg > sample-a2i-images/pexels-photo-{ind}.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_photos = ['sample-a2i-images/pexels-photo-980382.jpeg', # motorcycle\n",
    "               'sample-a2i-images/pexels-photo-276517.jpeg', # bicycle\n",
    "               'sample-a2i-images/pexels-photo-1571457.jpeg'] # sofa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results, detection_filtered, f = load_and_predict(test_photos[2], object_detector, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of 0.465 is considered quite low in modern computer vision and there is a mislabeling. This is due to the fact that the SSD model was under-trained for demonstration purposes in the [training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb). However this under-trained model serves as a perfect example of brining human reviewers when a model is unable to make a high confidence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating human review Workteam or Workforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workforce is the group of workers that you have selected to label your dataset. You can choose either the Amazon Mechanical Turk workforce, a vendor-managed workforce, or you can create your own private workforce for human reviews. Whichever workforce type you choose, Amazon Augmented AI takes care of sending tasks to workers. \n",
    "\n",
    "When you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to Amazon Augmented AI human review tasks. You can have multiple work teams and can assign one or more work teams to each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your Workteam, visit the instructions here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html\n",
    "\n",
    "After you have created your workteam, replace YOUR_WORKTEAM_ARN below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN = 'YOUR_WORKTEAM_ARN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html to add the necessary permissions to your role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to setup the rest of our clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import uuid\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker', region)\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "# Amazon S3 client \n",
    "s3 = boto3.client('s3', region)\n",
    "\n",
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-sagemaker-object-detection-demo-' + timestamp\n",
    "\n",
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-sagemaker-object-detection-demo-' + timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Create a human task UI resource, giving a UI template in liquid html. This template will be rendered to the human workers whenever human loop is required.\n",
    "\n",
    "For over 70 pre built UIs, check: https://github.com/aws-samples/amazon-a2i-sample-task-uis.\n",
    "\n",
    "We will be taking an [object detection UI](https://github.com/aws-samples/amazon-a2i-sample-task-uis/blob/master/images/bounding-box.liquid.html) and filling in the object categories in the `labels` variable in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "  <crowd-bounding-box\n",
    "    name=\"annotatedResult\"\n",
    "    src=\"{{ task.input.taskObject | grant_read_access }}\"\n",
    "    header=\"Draw bounding boxes around all the objects in this image\"\n",
    "    labels=\"['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\"\n",
    "  >\n",
    "    <full-instructions header=\"Bounding Box Instructions\" >\n",
    "      <p>Use the bounding box tool to draw boxes around the requested target of interest:</p>\n",
    "      <ol>\n",
    "        <li>Draw a rectangle using your mouse over each instance of the target.</li>\n",
    "        <li>Make sure the box does not cut into the target, leave a 2 - 3 pixel margin</li>\n",
    "        <li>\n",
    "          When targets are overlapping, draw a box around each object,\n",
    "          include all contiguous parts of the target in the box.\n",
    "          Do not include parts that are completely overlapped by another object.\n",
    "        </li>\n",
    "        <li>\n",
    "          Do not include parts of the target that cannot be seen,\n",
    "          even though you think you can interpolate the whole shape of the target.\n",
    "        </li>\n",
    "        <li>Avoid shadows, they're not considered as a part of the target.</li>\n",
    "        <li>If the target goes off the screen, label up to the edge of the image.</li>\n",
    "      </ol>\n",
    "    </full-instructions>\n",
    "\n",
    "    <short-instructions>\n",
    "      Draw boxes around the requested target of interest.\n",
    "    </short-instructions>\n",
    "  </crowd-bounding-box>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker_client.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Flow Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to create a flow definition definition. Flow Definitions allow us to specify:\n",
    "\n",
    "* The workforce that your tasks will be sent to.\n",
    "* The instructions that your workforce will receive. This is called a worker task template.\n",
    "* The configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks.\n",
    "* Where your output data will be stored.\n",
    "\n",
    "This demo is going to use the API, but you can optionally create this workflow definition in the console as well. \n",
    "\n",
    "For more details and instructions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workflow_definition_response = sagemaker_client.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= role,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Identify and locate the object in an image.\",\n",
    "            \"TaskTitle\": \"Object detection a2i demo\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker_client.describe_flow_definition(FlowDefinitionName=flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Human Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have setup our Flow Definition, we are ready to call our object detection endpoint on SageMaker and start our human loops. In this tutorial, we are interested in starting a HumanLoop only if the highest prediction probability score returned by our model for objects detected is less than 50%. \n",
    "\n",
    "So, with a bit of logic, we can check the response for each call to the SageMaker endpoint using `load_and_predict` helper function, and if the highest score is less than 50%, we will kick off a HumanLoop to engage our workforce for a human review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample images to s3 bucket for a2i UI to display\n",
    "!aws s3 sync ./sample-a2i-images/ s3://{BUCKET}/a2i-results/sample-a2i-images/\n",
    "    \n",
    "human_loops_started = []\n",
    "SCORE_THRESHOLD = .50\n",
    "for fname in test_photos:\n",
    "    # Call SageMaker endpoint and not display any object detected with probability lower than 0.4.\n",
    "    response, score_filtered, fig = load_and_predict(fname, object_detector, threshold=0.4)\n",
    "    # Sort by prediction score so that the first item has the highest probability\n",
    "    score_filtered.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Our condition for triggering a human review\n",
    "    if (score_filtered[0][1] < SCORE_THRESHOLD):\n",
    "        s3_fname='s3://%s/a2i-results/%s' % (BUCKET, fname)\n",
    "        print(s3_fname)\n",
    "        humanLoopName = str(uuid.uuid4())\n",
    "        inputContent = {\n",
    "            \"initialValue\": score_filtered[0][0],\n",
    "            \"taskObject\": s3_fname # the s3 object will be passed to the worker task UI to render\n",
    "        }\n",
    "        # start an a2i human review loop with an input\n",
    "        start_loop_response = a2i.start_human_loop(\n",
    "            HumanLoopName=humanLoopName,\n",
    "            FlowDefinitionArn=flowDefinitionArn,\n",
    "            HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(inputContent)\n",
    "            }\n",
    "        )\n",
    "        human_loops_started.append(humanLoopName)\n",
    "        print(f'Object detection Confidence Score of %s is less than the threshold of %.2f' % (score_filtered[0][0], SCORE_THRESHOLD))\n",
    "        print(f'Starting human loop with name: {humanLoopName}  \\n')\n",
    "    else:\n",
    "        print(f'Object detection Confidence Score of %s is above than the threshold of %.2f' % (score_filtered[0][0], SCORE_THRESHOLD))\n",
    "        print('No human loop created. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task\n",
    "Since we are using private workteam, we should go to the labling UI to perform the inspection ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker_client.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task Results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once work is completed, Amazon A2I stores results in your S3 bucket and sends a Cloudwatch event. Your results should be available in the S3 OUTPUT_PATH when all work is completed. Note that the human answer, the label and the bounding box, is returned and saved in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental training with SageMaker (Optional)\n",
    "Now that we have used the model to generate prediction on some random out-of-sample images and got unsatisfactory prediction (low probability). We also demonstrated how to use Amazon Augmented AI to review and label the image based on custom criteria. Next step in a typical machine learning life cycle is to include these cases with which the model has trouble in the next batch of training data for retraining purposes so that the model can now learn from a set of new training data to improve the model. In machine learning we call it [incremental training](https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html).\n",
    "\n",
    "There are [three ways](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html#object-detection-inputoutput) to supply the image data and annotation to SageMaker built-in object detection algorithm. We trained our original model with the [RecordIO format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) as we converted the PASCAL VOC images and annotations into RecordIO format. If you want to create a custom RecordIO data, you could follow the steps outlined [here](https://gluon-cv.mxnet.io/build/examples_datasets/detection_custom.html). Alternatively, SageMaker built-in object detection algorithm also takes JSON file as annotation along with your JPEG/PNG images. You could create one JSON file per image as in **Train with the Image Format** in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html#object-detection-inputoutput) and in this [example](https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_image_json_format.ipynb), or take an advantage of [pipe mode](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/) enabled by using [Augmented Manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html) as input format. Pipe mode accelerate overall model training time up to 35% by streaming the data into the training algorithm while it is running instead of copying data to the EBS volume attached to the training instance. We could construct augmented manifest file from the A2I output with the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories_dict = {str(i): j for i, j in enumerate(object_categories)}\n",
    "\n",
    "def convert_a2i_to_augmented_manifest(a2i_output):\n",
    "    annotations = []\n",
    "    confidence = []\n",
    "    for i, bbox in enumerate(a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['boundingBoxes']):\n",
    "        object_class_key = [key for (key, value) in object_categories_dict.items() if value == bbox['label']][0]\n",
    "        obj = {'class_id': int(object_class_key), \n",
    "               'width': bbox['width'],\n",
    "               'top': bbox['top'],\n",
    "               'height': bbox['height'],\n",
    "               'left': bbox['left']}\n",
    "        annotations.append(obj)\n",
    "        confidence.append({'confidence': 1})\n",
    "\n",
    "    # We set \"a2i-retraining\" as the attribute name for this dataset. This will later be used in setting the training data\n",
    "    augmented_manifest={'source-ref': a2i_output['inputContent']['taskObject'],\n",
    "                        'a2i-retraining': {'annotations': annotations,\n",
    "                                           'image_size': [{'width': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['width'],\n",
    "                                                           'depth':3,\n",
    "                                                           'height': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['height']}]},\n",
    "                        'a2i-retraining-metadata': {'job-name': 'a2i/%s' % a2i_output['humanLoopName'],\n",
    "                                                    'class-map': object_categories_dict,\n",
    "                                                    'human-annotated':'yes',\n",
    "                                                    'objects': confidence,\n",
    "                                                    'creation-date': a2i_output['humanAnswers'][0]['submissionTime'],\n",
    "                                                    'type':'groundtruth/object-detection'}}\n",
    "    return augmented_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take an A2I output json and result in a json object that is compatible to how Amazon SageMaker Ground Truth outputs the result and how SageMaker built-in object detection algorithm expects from the input. In order to create a cohort of training images from all the images re-labeled by human reviewers in A2I console. You can loop through all the A2I output, convert the json file, and concatenate them into a JSON Lines file, with each line represents results of one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "with open('augmented.manifest', 'w') as outfile:\n",
    "    # convert the a2i json to augmented manifest for each human loop output\n",
    "    for resp in completed_human_loops:\n",
    "        splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "        output_bucket_key = splitted_string[1]\n",
    "\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "        content = response[\"Body\"].read()\n",
    "        json_output = json.loads(content)\n",
    "        \n",
    "        # convert using the function\n",
    "        augmented_manifest = convert_a2i_to_augmented_manifest(json_output)\n",
    "        print(json.dumps(augmented_manifest))\n",
    "        json.dump(augmented_manifest, outfile)\n",
    "        outfile.write('\\n')\n",
    "        output.append(augmented_manifest)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how Json Lines looks like\n",
    "!head -n2 augmented.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the manifest file to S3\n",
    "!aws s3 cp augmented.manifest {OUTPUT_PATH}/augmented.manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training with Ground Truth output augmented manifest file outlined in this [blog](https://aws.amazon.com/blogs/machine-learning/easily-train-models-using-datasets-labeled-by-amazon-sagemaker-ground-truth/), once we have collected enough data points, we can construct a new `Estimator` for incremental training. \n",
    "\n",
    "For incremental training, the choice of hyperparameters becomes critical. Since we are continue the learning and optimization from the last model, an appropriate starting `learning_rate`, for example, would again need to be determined. But as a rule of thumb, even with the introduction of new, unseen data, we should start out the incremental training with a smaller `learning_rate` and different learning rate schedule (`lr_scheduler_factor` and `lr_scheduler_step`) than that of the previous training job as the optimization has previously reached to a more stable state with reduced learning rate. We should see a similar mAP performance on the original validation dataset in the first epoch in the incremental training. \n",
    "\n",
    "We here will be using the hyperparameters exactly the same as how the first model was trained in the [training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb), with the following exceptions\n",
    "\n",
    "- smaller learning rate (`learning_rate` was 0.001, now 0.0001)\n",
    "- using the weights from the trained model instead of pre-trained weights that comes with the algorithm (`use_pretrained_model=0`).\n",
    "\n",
    "Note that the following working code snippet is meant to demonstrate how to set up the A2I output for training in SageMaker with object detection algorithm. Incremental training with merely 1 or 2 new samples and untuned hyperparameters, would not yield a meaning model, if not experiencing [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference).\n",
    "\n",
    "*The next cell would take about 5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path definition\n",
    "s3_train_data = f'{OUTPUT_PATH}/augmented.manifest'\n",
    "# Reusing the training data for validation here for demonstration purposes\n",
    "# but in practice you should provide a set of data that you want to validate the training against\n",
    "s3_validation_data = s3_train_data \n",
    "s3_output_location = f'{OUTPUT_PATH}/incremental-training'\n",
    "\n",
    "num_training_samples = len(output)\n",
    "\n",
    "# Create a model object set to using \"Pipe\" mode because we are inputing augmented manifest files.\n",
    "new_od_model = sagemaker.estimator.Estimator(image, # same object detection image that we used for model hosting  \n",
    "                                             role, \n",
    "                                             instance_count=1, \n",
    "                                             instance_type='ml.p3.2xlarge', \n",
    "                                             volume_size = 50, \n",
    "                                             max_run = 360000, \n",
    "                                             input_mode = 'Pipe',\n",
    "                                             output_path=s3_output_location, \n",
    "                                             sagemaker_session=sess) \n",
    "\n",
    "# same set of hyperparameters from the original training job\n",
    "new_od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                                 use_pretrained_model=0, # we are going to use our own model\n",
    "                                 num_classes=20,\n",
    "                                 learning_rate=0.0001,   # smaller learning rate for a more stable search\n",
    "                                 mini_batch_size=1,\n",
    "                                 epochs=1,               # 1 for demo purposes\n",
    "                                 lr_scheduler_step='3,6',\n",
    "                                 lr_scheduler_factor=0.1,\n",
    "                                 optimizer='sgd',\n",
    "                                 momentum=0.9,\n",
    "                                 weight_decay=0.0005,\n",
    "                                 overlap_threshold=0.5,\n",
    "                                 nms_threshold=0.45,\n",
    "                                 image_shape=300,\n",
    "                                 label_width=350,\n",
    "                                 num_training_samples=num_training_samples)\n",
    "\n",
    "# setting the input data\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data, \n",
    "                                            distribution='FullyReplicated', \n",
    "                                            content_type='application/x-recordio',\n",
    "                                            record_wrapping='RecordIO',\n",
    "                                            s3_data_type='AugmentedManifestFile', \n",
    "                                            attribute_names=['source-ref', 'a2i-retraining'])\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data, \n",
    "                                                 distribution='FullyReplicated', \n",
    "                                                 content_type='application/x-recordio',\n",
    "                                                 record_wrapping='RecordIO',\n",
    "                                                 s3_data_type='AugmentedManifestFile', \n",
    "                                                 attribute_names=['source-ref', 'a2i-retraining'])\n",
    "\n",
    "# Use the output model from the original training job.  \n",
    "model_data = sagemaker.inputs.TrainingInput(model_data_s3_uri, \n",
    "                                            distribution='FullyReplicated',\n",
    "                                            content_type='application/x-sagemaker-model', \n",
    "                                            s3_data_type='S3Prefix',\n",
    "                                            input_mode = 'File')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'model': model_data}\n",
    "                 \n",
    "new_od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you would get a new model in the `s3_output_location`, you can deploy it to a new endpoint or modify an endpoint without taking models that are already deployed into production out of service. For example, you can add new model variants, update the ML Compute instance configurations of existing model variants, or change the distribution of traffic among model variants. To modify an endpoint, you provide a new endpoint configuration. Amazon SageMaker implements the changes without any downtime. For more information, see [UpdateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html) and [UpdateEndpointWeightsAndCapacities](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpointWeightsAndCapacities.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on incremental training\n",
    "It is recommended to perform a search over the hyperparameter space for your incremental training with [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) for an optimal set of hyperparameters, especially the ones related to learning rate: `learning_rate`, `lr_scheduler_factor` and `lr_scheduler_step` from the SageMaker object detection algorithm. We have an [example](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/image_classification_early_stopping/hpo_image_classification_early_stopping.ipynb) of running a hyperparameter tuning job using Amazon SageMaker Automatic Model Tuning feature. Please try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End, but....!\n",
    "This is the end of the example. Remember to execute the next cell to delete the endpoint otherwise it will continue to incur charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
