{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting objects unique to your business using Amazon Rekognition Custom Labels and sending predictions for human review using Amazon A2I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a custom model to analyze images is a significant undertaking that requires time, expertise, and resources. It often takes months to complete. Additionally, it can require thousands or tens of thousands of hand-labeled images to provide the model with enough data to accurately make decisions. Generating this data can take months to gather, and can require large teams of labelers to prepare it for use in machine learning. In addition setting up a workflow for auditing or reviewing model predictions to validate adherence to your requirements can further add to the overall complexity.  \n",
    "\n",
    "With Amazon Rekognition Custom Labels, you can identify the objects and scenes in images that are specific to your business needs. For example, you can find your logo in social media posts, identify your products on store shelves, classify machine parts in an assembly line, distinguish healthy and infected plants, or detect animated characters in videos. Amazon Rekognition Custom Labels builds off of Amazon Rekognitionâ€™s existing capabilities, which are already trained on tens of millions of images across many categories. Instead of thousands of images, you can upload a small set of training images (typically a few hundred images or less) that are specific to your use case. Predictions from Amazon Rekognition Custom Labels can be easily sent to Amazon Augmented AI (Amazon A2I). Amazon A2I makes it easy to integrate a human review into your machine learning workflow. This allows you to automatically have humans step into your ML pipeline to review results below a confidence threshold, for setting up review/auditing workflows and to augment the prediction results to improve model accuracy. \n",
    "\n",
    "In this post we show you to how to build a custom object detection model trained to detect pepperoni slices in a pizza using Amazon Rekognition custom labels with a dataset labeled using Amazon SageMaker GroundTruth. We then show how to create your own private workforce and setup an Amazon A2I workflow definition to conditionally trigger human loops for review and augmenting tasks. You can use the annotations created by Amazon A2I for model re-training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Prerequisites](#Prequisites)\n",
    "2. [Step 1 - Train an Amazon Rekognition custom model](#Step-2---Train-a-Rekognition-custom-model)\n",
    "3. [Step 2 - Setup an Amazon A2I Flow Definition ](#Step-3---Setup-an-Amazon-A2I-Flow-Definition)\n",
    "4. [Step 3 - Start Human Loops](#Step-4---Start-Human-Loops)\n",
    "5. [Step 4 - Evaluate Results](#Step-4---Evaluate-Results)\n",
    "6. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before getting started, you need to create your human workforce, set up your Amazon SageMaker Studio notebook and download the datasets we will use in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your human workforce\n",
    "This step requires you to use the AWS Console. We will create a private workteam and add only one user (you) to it. To create a private team:\n",
    "\n",
    "1. Go to AWS Console > Amazon SageMaker > Labeling workforces\n",
    "1. Click \"Private\" and then \"Create private team\".\n",
    "1. Enter the desired name for your private workteam.\n",
    "1. Enter your own email address in the \"Email addresses\" section.\n",
    "1. Enter the name of your organization and a contact email to administer the private workteam.\n",
    "1. Click \"Create Private Team\".\n",
    "1. The AWS Console should now return to AWS Console > Amazon SageMaker > Labeling workforces. Your newly created team should be visible under \"Private teams\". Next to it you will see an ARN which is a long string that looks like arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name. Please enter this ARN in the cell below and execute the cell.\n",
    "1. You should get an email from no-reply@verificationemail.com that contains your workforce username and password.\n",
    "1. In AWS Console > Amazon SageMaker > Labeling workforces, click on the URL in Labeling portal sign-in URL. Use the email/password combination from Step 8 to log in (you will be asked to create a new, non-default password).\n",
    "1. This is your private worker's interface. When we create a verification task in Verify your task using a private team below, your task should appear in this window. You can invite your colleagues to participate in the labeling job by clicking the \"Invite new workers\" button.\n"
   ]
  },
  {
   "source": [
    "### Initialize Variables\n",
    "\n",
    "Use the following cell to specify:\n",
    "* `WORKTEAM_ARN` : the Amazon Resource Name (ARN) of the private work team you want to use for this walkthrough.\n",
    "* `BUCKET` : The Amazon S3 bucket you want to use to store input and output data. This bucket must have CORS enabled (see note below).\n",
    "*  `REGION` : The AWS Region your notebook instance, `BUCKET` and work team are located in. Note that these resources must be located in the same AWS Region.\n",
    "\n",
    "**Important**: The bucket you specify for `BUCKET` must have CORS enabled. You can enable CORS by adding a policy similar to the following to your Amazon S3 bucket. To learn how to add CORS to an S3 bucket, see [CORS Permission Requirement](https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html#a2i-cors-update) in the Amazon A2I documentation. \n",
    "\n",
    "\n",
    "```\n",
    "[{\n",
    "   \"AllowedHeaders\": [],\n",
    "   \"AllowedMethods\": [\"GET\"],\n",
    "   \"AllowedOrigins\": [\"*\"],\n",
    "   \"ExposeHeaders\": []\n",
    "}]\n",
    "```\n",
    "\n",
    "If you do not add a CORS configuration to the S3 buckets that contains your image input data, human review tasks for those input data objects will fail. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import botocore\n",
    "from time import gmtime, strftime \n",
    "\n",
    "REGION = 'us-east-1'\n",
    "WORKTEAM_ARN= 'your-workteam-arn'\n",
    "BUCKET = 'your-s3-bucket-name'\n",
    "PREFIX = 'a2i-rekogCL-demo-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "s3 = boto3.client('s3')\n",
    "s3r = boto3.resource('s3')\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "ROLE = get_execution_role()\n",
    "display(ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Amazon SageMaker GroundTruth object detection manifest to Amazon S3\n",
    "For our post we will use a pre-labeled object detection dataset created using the Amazon SageMaker GroundTruth bounding box labeling job task type. The image files from this dataset are available in the **data/images** folder and the manifest file is available in the **data/manifest** folder in this repo. Let's Ddownload the images and the manifest to a S3 bucket that we will use for training our Amazon Rekognition custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload images\n",
    "folderpath = r\"data/images\" # make sure to put the 'r' in front and provide the folder where your files are\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories\n",
    "for path in filepaths:\n",
    "    s3r.meta.client.upload_file(path, BUCKET, PREFIX+'/'+path)\n",
    "#upload test image in s3\n",
    "folderpath = r\"data/testimages\" # make sure to put the 'r' in front and provide the folder where your files are\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories\n",
    "for path in filepaths:\n",
    "    s3r.meta.client.upload_file(path, BUCKET, PREFIX+'/'+path)\n",
    "\n",
    "# replace bucket, prefix entries from the template and upload the manifest file\n",
    "tempname = (\"bucket\",\"prefix\")\n",
    "realname = (BUCKET,PREFIX)\n",
    "f1 = open('./data/manifest/output-change.manifest', 'r')\n",
    "f2 = open('./data/manifest/output.manifest', 'w')\n",
    "for line in f1:\n",
    "    for check, rep in zip(tempname, realname):\n",
    "        line = line.replace(check, rep)\n",
    "    f2.write(line)\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "s3r.meta.client.upload_file('./data/manifest/output.manifest', BUCKET, PREFIX+'/'+'data/manifest/output.manifest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Train an Amazon Rekognition custom model\n",
    "\n",
    "We will perform the steps to train the custom object detection model using the AWS console. Let's begin by selecting the [Amazon Rekognition custom labels in the AWS Console](https://console.aws.amazon.com/rekognition/custom-labels#/). The Amazon Rekognition Custom Labels console is where you create and manage your models. The first time you use the console, Amazon Rekognition Custom Labels asks to create an Amazon S3 bucket in your account. The bucket is used to store your Amazon Rekognition Custom Labels projects, datasets, and models. You can't use the Amazon Rekognition Custom Labels console unless the bucket is created. **Please note** that this bucket is not the same as the S3 bucket you defined for this notebook in the Prerequisites section. \n",
    "\n",
    "### Create a Project\n",
    "\n",
    "Within Amazon Rekognition Custom Labels, you use projects to manage the models that you create. A project manages the input images and labels, datasets, model training, model evaluation, and running the model. For more information, see Creating an Amazon Rekognition Custom Labels Project (https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cp-create-project.html).\n",
    "\n",
    "### Create a dataset\n",
    "\n",
    "In an Amazon Rekognition Custom Labels project, datasets contain the images, assigned labels, and bounding boxes that you use to train and test a custom model. You can create and manage datasets by using the Amazon Rekognition Custom Labels console. You can't create a dataset with the Amazon Rekognition Custom Labels API. For our model training, we already have the pre-labeled images from Amazon SageMaker GroundTruth that we need to create our training dataset. We will use the images and the manifest file we uploaded to the S3 bucket in the Prerequisites section. For Image Location, please select \"Import Images labeled by Amazon SageMaker GroundTruth\" and provide the S3 path to the manifest file you uploaded in the cell above. \n",
    "\n",
    "**Note:** You should get a prompt to provide the S3 bucket policy when you provide the S3 path above as shown here. Please copy the bucket policy as requested:\n",
    "* Make sure that your S3 bucket is correctly configured\n",
    "* You've specified an external S3 bucket: your bucket name\n",
    "* To use the images in this bucket, copy the policy below (to copy, choose the preceding link text). \n",
    "* Paste the policy into the \"Bucket Policy\" section of your bucket.\n",
    "\n",
    "### Train an Amazon Rekognition Custom Model\n",
    "\n",
    "Training a model requires labeled images and algorithms to train the model. Amazon Rekognition Custom Labels simplifies this process by choosing the algorithm for you. Accurately labeled images are very important to the process because they influence the algorithm that is chosen to train the model. After training a model, you can evaluate its performance. For more information, see Evaluating a Trained Amazon Rekognition Custom Labels Model (https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/tr-train-results.html).\n",
    "\n",
    "To train an Amazon Rekognition Custom Model\n",
    "* Choose Train Model.\n",
    "* For Choose project, choose your newly created project.\n",
    "* For Choose training dataset, choose your newly created dataset.\n",
    "* For Choose a test dataset, provide the training dataset you created above\n",
    "* Click on Train at the bottom right of the page\n",
    "    \n",
    "The training should approximately take 45 minutes to complete. When training completes, you can evaluate the performance of the model. To help you, Amazon Rekognition Custom Labels provides summary metrics and evaluation metrics for each label. For information about the available metrics, see [Metrics for Evaluating Your Model](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/tr-metrics-use.html). To improve your model using metrics, see [Improving an Amazon Rekognition Custom Labels Model](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/tr-improve-model.html). For more details please refer to the [Amazon Rekognition Custom Labels documentation](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/tm-train-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the status of our Rekognition custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws rekognition describe-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the project-arn below with the project-arn of your project from the describe-projects output above\n",
    "!aws rekognition describe-project-versions --project-arn 'your-project-arn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Project so we can run object detection using this model\n",
    "# Copy/paste the ProjectVersionArn for your model from the describe-project-versions cell output above to the --project-version-arn parameter here\n",
    "!aws rekognition start-project-version \\\n",
    "  --project-version-arn 'your-project-version-arn' \\\n",
    "  --min-inference-units 1 \\\n",
    "  --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets run object detection using our Rekogntion custom model using one of the images from our dataset\n",
    "Please copy the ProjectVersionArn for your model from the result of the cell execution above and provide this as input to the model_arn below. Lets select a sample image from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ExifTags, ImageColor, ImageFont\n",
    "image=Image.open('./data/testimages/pexels-polina-tankilevitch-4109078.jpg')\n",
    "display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets send this image to our Amazon Rekognition Custom model for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "test_photo = '/data/testimages/pexels-polina-tankilevitch-4109078.jpg'\n",
    "#s3_connection = boto3.resource('s3')\n",
    "client = boto3.client('rekognition')\n",
    "s3_object = s3r.Object(BUCKET, PREFIX + test_photo)\n",
    "s3_response = s3_object.get()\n",
    "\n",
    "stream = io.BytesIO(s3_response['Body'].read())\n",
    "image = Image.open(stream)\n",
    "model_arn = 'your-project-version-arn'\n",
    "min_confidence=50    \n",
    "#Call DetectCustomLabels \n",
    "response = client.detect_custom_labels(Image={'S3Object': {'Bucket': BUCKET, 'Name': PREFIX + test_photo}},\n",
    "    MinConfidence=min_confidence,\n",
    "    ProjectVersionArn=model_arn)\n",
    "#print(\"Response from Rekog is: \" + str(response))   \n",
    "imgWidth, imgHeight = image.size  \n",
    "draw = ImageDraw.Draw(image)  \n",
    "       \n",
    "# calculate and display bounding boxes for each detected custom label       \n",
    "   \n",
    "for customLabel in response['CustomLabels']:\n",
    "    print('Label ' + str(customLabel['Name'])) \n",
    "    print('Confidence ' + str(customLabel['Confidence'])) \n",
    "    if 'Geometry' in customLabel:\n",
    "        box = customLabel['Geometry']['BoundingBox']\n",
    "        left = imgWidth * box['Left']\n",
    "        top = imgHeight * box['Top']\n",
    "        width = imgWidth * box['Width']\n",
    "        height = imgHeight * box['Height']\n",
    "\n",
    "        fnt = ImageFont.load_default()\n",
    "        draw.text((left,top), customLabel['Name'], fill='#00d400', font=fnt) \n",
    "\n",
    "        print('Left: ' + '{0:.0f}'.format(left))\n",
    "        print('Top: ' + '{0:.0f}'.format(top))\n",
    "        print('Label Width: ' + \"{0:.0f}\".format(width))\n",
    "        print('Label Height: ' + \"{0:.0f}\".format(height))\n",
    "        print(\"Label %s\",json.dumps(customLabel['Name']))\n",
    "\n",
    "        points = (\n",
    "            (left,top),\n",
    "            (left + width, top),\n",
    "            (left + width, top + height),\n",
    "            (left , top + height),\n",
    "            (left, top))\n",
    "        draw.line(points, fill='#00d400', width=5)\n",
    "\n",
    "display(image)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Setup an Amazon A2I Flow Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Augmented AI (Amazon A2I) makes it easy to build the workflows required for human review of ML predictions. Amazon A2I brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers.\n",
    "\n",
    "To incorporate Amazon A2I into your human review workflows you need:\n",
    "\n",
    "- A worker task template to create a worker UI. The worker UI displays your input data, such as documents or images, and instructions to workers. It also provides interactive tools that the worker uses to complete your tasks. For more information, see A2I instructions overview\n",
    "\n",
    "- A human review workflow, also referred to as a flow definition. You use the flow definition to configure your human workforce and provide information about how to accomplish the human review task. To learn more see create flow definition\n",
    "\n",
    "- When using a custom task type, you start a human loop using the Amazon Augmented AI Runtime API. When you call StartHumanLoop in your custom application, a task is sent to human reviewers.\n",
    "\n",
    "In this section, you set up a human review loop for low-confidence detections in Amazon A2I. It includes the following steps:\n",
    "\n",
    "- Create a human task UI\n",
    "- Create the flow definition\n",
    "\n",
    "Lets now initialize some variables that we need in the subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# Amazon SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-rekog-custom-' + timestamp\n",
    "\n",
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-rekog-custom-' + timestamp\n",
    "\n",
    "# Flow definition outputs\n",
    "OUTPUT_PATH = f's3://{BUCKET}/{PREFIX}/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Create a human task UI resource, giving a UI template in liquid html. This template will be rendered to the human workers whenever human loop is required.\n",
    "\n",
    "For over 70 pre built UIs, check: https://github.com/aws-samples/amazon-a2i-sample-task-uis.\n",
    "\n",
    "We will be taking an [object detection UI](https://github.com/aws-samples/amazon-a2i-sample-task-uis/blob/master/images/bounding-box.liquid.html) and filling in the object categories in the `labels` variable in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "  <crowd-bounding-box\n",
    "    name=\"annotatedResult\"\n",
    "    src=\"{{ task.input.taskObject | grant_read_access }}\"\n",
    "    header=\"Identify Pepperoni Pizza slices in the image, select the label from the right and draw bounding boxes depicting them. You need one bounding box per pizza slice you want to label.\"\n",
    "    labels=\"['pepperoni pizza slice']\"\n",
    "  >\n",
    "    <full-instructions header=\"Bounding Box Instructions\" >\n",
    "      <p>Use the bounding box tool to draw boxes around the requested target of interest:</p>\n",
    "      <ol>\n",
    "        <li>Draw a rectangle using your mouse over each instance of the target.</li>\n",
    "        <li>Make sure the box does not cut into the target, leave a 2 - 3 pixel margin</li>\n",
    "        <li>\n",
    "          When targets are overlapping, draw a box around each object,\n",
    "          include all contiguous parts of the target in the box.\n",
    "          Do not include parts that are completely overlapped by another object.\n",
    "        </li>\n",
    "        <li>\n",
    "          Do not include parts of the target that cannot be seen,\n",
    "          even though you think you can interpolate the whole shape of the target.\n",
    "        </li>\n",
    "        <li>Avoid shadows, they're not considered as a part of the target.</li>\n",
    "        <li>If the target goes off the screen, label up to the edge of the image.</li>\n",
    "      </ol>\n",
    "    </full-instructions>\n",
    "\n",
    "    <short-instructions>\n",
    "      Draw boxes around the requested target of interest.\n",
    "    </short-instructions>\n",
    "  </crowd-bounding-box>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker_client.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Flow Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to create a flow definition definition. Flow Definitions allow us to specify:\n",
    "\n",
    "* The workforce that your tasks will be sent to.\n",
    "* The instructions that your workforce will receive. This is called a worker task template.\n",
    "* The configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks.\n",
    "* Where your output data will be stored.\n",
    "\n",
    "This notebook is going to use the API, but you can optionally create this workflow definition in the console as well. \n",
    "\n",
    "For more details and instructions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workflow_definition_response = sagemaker_client.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= ROLE,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Identify custom labels in the image\",\n",
    "            \"TaskTitle\": \"Identify custom image\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker_client.describe_flow_definition(FlowDefinitionName=flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Start Human Loops\n",
    "\n",
    "In this step we send our test dataset for predictions using our Amazon Rekognition Custom Labels model, and conditionally send a list of prediction results to a private workforce we setup for human review. We use the confidence score from the CustomLabels tag returned by the DetectCustomLabels API to send all images with a confidence score of less than 60% to the human loop. We execute the following tasks:\n",
    "\n",
    "- Trigger conditions for human loop activation\n",
    "- Check the human loop status and wait for reviewers to complete the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model predictions and trigger human loop\n",
    "We first ask Amazon Rekognition Custom Label model to send us labels it was able to detect with a confidence of greater than 20%. For those images that meet this criteria we then select the prediction results returned with a confidence of < 60% to a human loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "human_loops_started = []\n",
    "SCORE_THRESHOLD = 60\n",
    "\n",
    "folderpath = r\"data/testimages\" # make sure to put the 'r' in front and provide the folder where your files are\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories\n",
    "for path in filepaths:\n",
    "    # Call custom label endpoint and not display any object detected with probability lower than 0.01\n",
    "    response = client.detect_custom_labels(Image={'S3Object': {'Bucket': BUCKET, 'Name': PREFIX+'/'+path}},\n",
    "        MinConfidence=0,\n",
    "        ProjectVersionArn=model_arn)    \n",
    "  \n",
    "    #Get the custom labels\n",
    "    labels=response['CustomLabels']\n",
    "    if labels and labels[0]['Confidence'] < SCORE_THRESHOLD: \n",
    "        s3_fname='s3://%s/%s' % (BUCKET, PREFIX+'/'+path)\n",
    "        print(\"Images with labels less than 60% confidence score: \" + s3_fname)\n",
    "        humanLoopName = str(uuid.uuid4())\n",
    "        inputContent = {\n",
    "            \"initialValue\": labels[0]['Confidence'],\n",
    "            \"taskObject\": s3_fname\n",
    "        }\n",
    "        start_loop_response = a2i.start_human_loop(\n",
    "            HumanLoopName=humanLoopName,\n",
    "            FlowDefinitionArn=flowDefinitionArn,\n",
    "            HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(inputContent)\n",
    "            }\n",
    "        )\n",
    "        human_loops_started.append(humanLoopName)\n",
    "        print(f'Starting human loop with name: {humanLoopName}  \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker_client.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Evaluate results and re-train the model \n",
    "\n",
    "Once our human workforce has completed the tasks, Amazon A2I stores the annotation results in a S3 bucket. You can use these annotation results to updated your original labeling annotations, update your training dataset and re-train your models to improve their accuracy. You can also send the Amazon A2I annotations for usage in downstream analytics of prediction results, for auditing purposes and for conditional activation of application logic as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an augmented manifest for re-training\n",
    "\n",
    "We will now take the output from Amazon A2I and convert this to an **augmented manifest file** (similar to the **`output.manifest`** file we used for originally training our Amazon Rekognition Custom Labels model). Replace the **`dsname`** variable below with the name of your dataset that you created originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['pepperoni pizza slice','cheese slice'] # if you have more labels, add them here\n",
    "object_categories_dict = {str(i): j for i, j in enumerate(object_categories)}\n",
    "\n",
    "dsname = 'pepperoni_pizza'\n",
    "\n",
    "def convert_a2i_to_augmented_manifest(a2i_output):\n",
    "    annotations = []\n",
    "    confidence = []\n",
    "    for i, bbox in enumerate(a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['boundingBoxes']):\n",
    "        object_class_key = [key for (key, value) in object_categories_dict.items() if value == bbox['label']][0]\n",
    "        obj = {'class_id': int(object_class_key), \n",
    "               'width': bbox['width'],\n",
    "               'top': bbox['top'],\n",
    "               'height': bbox['height'],\n",
    "               'left': bbox['left']}\n",
    "        annotations.append(obj)\n",
    "        confidence.append({'confidence': 1})\n",
    "\n",
    "    # Change the attribute name to the dataset-name_BB for this dataset. This will later be used in setting the training data\n",
    "    augmented_manifest={'source-ref': a2i_output['inputContent']['taskObject'],\n",
    "                        dsname+'_BB': {'annotations': annotations,\n",
    "                                           'image_size': [{'width': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['width'],\n",
    "                                                           'depth':3,\n",
    "                                                           'height': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['height']}]},\n",
    "                        dsname+'_BB-metadata': {'job-name': 'a2i/%s' % a2i_output['humanLoopName'],\n",
    "                                                    'class-map': object_categories_dict,\n",
    "                                                    'human-annotated':'yes',\n",
    "                                                    'objects': confidence,\n",
    "                                                    'creation-date': a2i_output['humanAnswers'][0]['submissionTime'],\n",
    "                                                    'type':'groundtruth/object-detection'}}\n",
    "    return augmented_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take an A2I output json and result in a json object that is compatible to how Amazon SageMaker Ground Truth outputs the result and how the Amazon Rekognition Custom Labels expects from the input. In order to create a cohort of training images from all the images re-labeled by human reviewers in A2I console. You can loop through all the A2I output, convert the json file, and concatenate them into a JSON Lines file, with each line represents results of one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "with open('augmented-temp.manifest', 'w') as outfile:\n",
    "    # convert the a2i json to augmented manifest for each human loop output\n",
    "    for resp in completed_human_loops:\n",
    "        splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "        output_bucket_key = splitted_string[1]\n",
    "\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "        content = response[\"Body\"].read()\n",
    "        json_output = json.loads(content)\n",
    "        \n",
    "        # convert using the function\n",
    "        augmented_manifest = convert_a2i_to_augmented_manifest(json_output)\n",
    "        print(json.dumps(augmented_manifest))\n",
    "        json.dump(augmented_manifest, outfile)\n",
    "        outfile.write('\\n')\n",
    "        output.append(augmented_manifest)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the contents of the original output.manifest file to intentionally replace the image references whose annotations were augmented by Amazon A2I. This will create a new full copy of the manifest to use to train a 2nd model that trains on annotations from A2I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4 = open('./augmented-temp.manifest', 'r')\n",
    "with open('augmented.manifest', 'w') as outfl:\n",
    "    for lin1 in f4:\n",
    "        z_json = json.loads(lin1)\n",
    "        done_json = json.loads(lin1)\n",
    "        done_json['source-ref'] = 'a'\n",
    "        f3 = open('./data/manifest/output.manifest', 'r')\n",
    "        for lin2 in f3:\n",
    "            x_json = json.loads(lin2)\n",
    "            if z_json['source-ref'] == x_json['source-ref']:\n",
    "                print(\"replacing the annotations\")\n",
    "                x_json[dsname+'_BB'] = z_json[dsname+'_BB']\n",
    "                x_json[dsname+'_BB-metadata'] = z_json[dsname+'_BB-metadata']\n",
    "            elif done_json['source-ref'] != z_json['source-ref']:\n",
    "                print(\"This is a net new annotation to augmented file\")\n",
    "                json.dump(z_json, outfl)\n",
    "                outfl.write('\\n')\n",
    "                print(str(z_json))\n",
    "                done_json = z_json\n",
    "            json.dump(x_json, outfl)\n",
    "            outfl.write('\\n')         \n",
    "        f3.close()       \n",
    "f4.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how Json Lines looks like\n",
    "!head -n2 augmented.manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a new model using the augmented manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the manifest file to S3\n",
    "s3r.meta.client.upload_file('./augmented.manifest', BUCKET, PREFIX+'/'+'data/manifest/augmented.manifest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have uploaded the augmented manifest file from Amazon A2I to the S3 bucket, you can train a new model by using this augmented manifest as an input dataset. Please find below the list of steps you will perform:\n",
    "\n",
    "**`Create a new Amazon Rekognition Custom Labels dataset`**\n",
    "- Go to AWS Console --> Amazon Rekognition Custom Labels --> Datasets\n",
    "- Choose **Create dataset**, enter a new Dataset name and select *Import images labeled by Amazon Sagemaker Ground Truth*. Provide the S3 location of the augmented.manifest file you created above in `<S3 bucket location of your .manifest file>`\n",
    "- Create the dataset\n",
    "\n",
    "**`Train a new model by selecting this newly created dataset`**\n",
    "- Go to AWS Console --> Amazon Rekognition Custom Labels --> Projects\n",
    "- Click on the project you used when training your model the first time\n",
    "- Click Train Model and select the dataset you created in the step above\n",
    "- Click Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "To avoid incurring unnecessary charges, delete the resources used in this walkthrough\n",
    "when not in use, including the following:\n",
    "\n",
    "* [Amazon Rekognition Custom Label Project](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cp-delete.html)\n",
    "* [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/delete-bucket.html)\n",
    "* [Amazon A2I Flow definition](https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-delete-flow-definition.html)\n",
    "* [Amazon SageMaker notebook instance](https://sagemaker-workshop.com/cleanup/sagemaker.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws rekognition stop-project-version --project-version-arn 'your-project-version-arn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws rekognition delete-project --project-arn 'your-project-arn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-flow-definition --flow-definition-name flowDefinitionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated how you can use Amazon Rekognition Custom Labels and Amazon A2I to train models to detect objects and scenes unique to your business and define conditions to send the predictions to a Human Workflow with labelers to review and update the results. The human labeled output can be used to augment the training dataset for re-training, improving model accuracy or it can be sent to downstream applications for analytics and insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}