{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Augmented AI (Amazon A2I) integration with Amazon Rekognition [Example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit https://github.com/aws-samples/amazon-a2i-sample-jupyter-notebooks for all A2I Sample Notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "    1. [Supported Regions](#Supported-Regions)\n",
    "    2. [Workteam](#Workteam)\n",
    "    3. [Notebook Permission](#Notebook-Permission)\n",
    "3. [Client Setup](#Client-Setup)\n",
    "4. [Sample Data](#Sample-Data)\n",
    "    1. [Download sample images](#Download-sample-images)\n",
    "    2. [Upload images to S3](#Upload-images-to-S3)\n",
    "5. [Create Control Plane Resources](#Create-Control-Plane-Resources)\n",
    "    1. [Create Human Task UI](#Create-Human-Task-UI)\n",
    "    2. [Create Flow Definition](#Create-Flow-Definition)\n",
    "6. [Scenario 1: When Activation Conditions are met, and a Human Loop is created](#Scenario-1-:-When-Activation-Conditions-are-met-,-and-HumanLoop-is-created)\n",
    "    1. [Check Status of Human Loop](#Check-Status-of-Human-Loop)\n",
    "    2. [Wait For Workers to Complete Task](#Wait-For-Workers-to-Complete-Task)\n",
    "    3. [Check Status of Human Loop](#Check-Status-of-Human-Loop)\n",
    "    4. [View Task Results](#View-Task-Results)\n",
    "7. [Scenario 2: When Activation Conditions are not met, and a Human Loop is not created](#When-Activation-Conditions-are-not-met-,-and-HumanLoop-is-not-created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon Augmented AI (Amazon A2I) makes it easy to build the workflows required for human review of ML predictions. Amazon A2I brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers. \n",
    "\n",
    "Amazon A2I provides built-in human review workflows for common machine learning use cases, such as content moderation and text extraction from documents, which allows predictions from Amazon Rekognition and Amazon Textract to be reviewed easily. You can also create your own workflows for ML models built on Amazon SageMaker or any other tools. Using Amazon A2I, you can allow human reviewers to step in when a model is unable to make a high confidence prediction or to audit its predictions on an on-going basis. Learn more here: https://aws.amazon.com/augmented-ai/\n",
    "\n",
    "In this tutorial, we will show how you can use Amazon A2I directly within your API calls to Amazon Rekognition's Detect Moderation Labels API. \n",
    "\n",
    "For more in depth instructions, visit https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate Amazon A2I into your human review workflows, you need three resources:\n",
    "\n",
    "* A **worker task template** to create a worker UI. The worker UI displays your input data, such as documents or images, and instructions to workers. It also provides interactive tools that the worker uses to complete your tasks. For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-instructions-overview.html\n",
    "\n",
    "* A **human review workflow**, also referred to as a flow definition. You use the flow definition to configure your human workforce and provide information about how to accomplish the human review task. For built-in task types, you also use the flow definition to identify the conditions under which a review human loop is triggered. For example, with Amazon Rekognition can perform image content moderation using machine learning. You can use the flow definition to specify that an image will be sent to a human for content moderation review if Amazon Rekognition's confidence score output is low for your use case. You can create a flow definition in the Amazon Augmented AI console or with the Amazon A2I APIs. To learn more about both of these options, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html\n",
    "\n",
    "* A **human loop** to start your human review workflow. When you use one of the built-in task types, the corresponding AWS service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met or for each object if no conditions were specified. When a human loop is triggered, human review tasks are sent to the workers as specified in the flow definition.\n",
    "\n",
    "When using a custom task type, you start a human loop using the Amazon Augmented AI Runtime API. When you call StartHumanLoop in your custom application, a task is sent to human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Latest SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the latest installations of our dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 --upgrade\n",
    "!pip install -U botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need to set up the following data:\n",
    "* `region` - Region to call A2I. Must be us-east-1 for Preview\n",
    "* `bucket` - A S3 bucket accessible by the given role\n",
    "    * Used to store the sample images & output results\n",
    "    * Must be within the same region A2I is called from\n",
    "* `role` - The IAM role used as part of StartHumanLoop. By default, this notebook will use the execution role\n",
    "* `workteam` - Group of people to send the work to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Regions\n",
    "\n",
    "For the Amazon A2I Preview, the **us-east-1** AWS region is supported. Please ensure that any accompanying resources are created in that region, including S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2I is in us-east-1 for Preview\n",
    "REGION = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Bucket and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "BUCKET = '<YOUR_BUCKET>'\n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Permission\n",
    "\n",
    "The AWS IAM Role used to execute the notebook needs to have the following permissions:\n",
    "\n",
    "* RekognitionFullAccess\n",
    "* SagemakerFullAccess\n",
    "* S3 Read Access to the bucket listed below\n",
    "* AmazonSageMakerMechanicalTurkAccess (if using MechanicalTurk as your Workforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "ROLE = get_execution_role()\n",
    "display(ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html to add the necessary permissions to your role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workteam or Workforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workforce is the group of workers that you have selected to label your dataset. You can choose either the Amazon Mechanical Turk workforce, a vendor-managed workforce, or you can create your own private workforce for human reviews. Whichever workforce type you choose, Amazon Augmented AI takes care of sending tasks to workers. \n",
    "\n",
    "When you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to Amazon Augmented AI human review tasks. You can have multiple work teams and can assign one or more work teams to each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your Workteam, visit the instructions here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html\n",
    "\n",
    "After you have created your workteam, replace YOUR_WORKTEAM_ARN below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN= \"<YOUR_WORKTEAM>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to setup the clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', REGION)\n",
    "\n",
    "# Amazon Rekognition client\n",
    "rekognition = boto3.client('rekognition', REGION)\n",
    "\n",
    "# Amazon Augmented AI (A2I) Runtime client\n",
    "a2i_runtime_client = boto3.client('sagemaker-a2i-runtime', REGION)\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3', REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Pretty print setup\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "# Function to pretty-print AWS SDK responses\n",
    "def print_response(response):\n",
    "    if 'ResponseMetadata' in response:\n",
    "        del response['ResponseMetadata']\n",
    "    pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll be using sample images from Amazon Rekognition image moderation console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images\n",
    "!wget 'https://dhei5unw3vrsx.cloudfront.net/images/family_picnic_resized.jpg' -O 'family_picnic_resized.jpg'\n",
    "!wget 'https://dhei5unw3vrsx.cloudfront.net/images/yoga_swimwear_resized.jpg' -O 'yoga_swimwear_resized.jpg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoga swimwear <img src=\"yoga_swimwear_resized.jpg\" alt=\"Yoga Swimwear\" width=\"400\" />\n",
    "Family picnic <img src=\"family_picnic_resized.jpg\" alt=\"Family picnic\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload images to S3\n",
    "\n",
    "Upload the sample images to your S3 bucket. They will be read by Amazon Rekognition and A2I later when human task is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageName1 = 'yoga_swimwear_resized.jpg'\n",
    "imageName2 = 'family_picnic_resized.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(imageName1, BUCKET, imageName1)\n",
    "s3.upload_file(imageName2, BUCKET, imageName2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check the S3 bucket BUCKET that it contains the images at the specified key paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources\n",
    "\n",
    "Here we'll be constructing the following control plane resources: Human Task UI and Flow Definition, using the SageMaker CreateTaskUI and CreateFlowDefinition APIs, respectively.\n",
    "\n",
    "These resources can be created once and used to drive any subsequent A2I human loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Create a human task UI resource, giving a UI template in liquid html. This template will be rendered to the workers whenever a human loop is created.\n",
    "\n",
    "Amazon A2I provides pre-built templates for Amazon Rekognition and Amazon Tetract. Additionally, over 70  different samples UIs for different use cases are here: https://github.com/aws-samples/amazon-a2i-sample-task-uis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "{% capture s3_arn %}http://s3.amazonaws.com/{{ task.input.aiServiceRequest.image.s3Object.bucket }}/{{ task.input.aiServiceRequest.image.s3Object.name }}{% endcapture %}\n",
    "\n",
    "<crowd-form>\n",
    "  <crowd-rekognition-detect-moderation-labels\n",
    "    categories='[\n",
    "      {% for label in task.input.selectedAiServiceResponse.moderationLabels %}\n",
    "        {\n",
    "          name: \"{{ label.name }}\",\n",
    "          parentName: \"{{ label.parentName }}\",\n",
    "        },\n",
    "      {% endfor %}\n",
    "    ]'\n",
    "    src=\"{{ s3_arn | grant_read_access }}\"\n",
    "    header=\"Please select all categories that apply\"\n",
    "  >\n",
    "    <short-instructions header=\"Instructions\">\n",
    "      <style>\n",
    "        .instructions {\n",
    "          white-space: pre-wrap;\n",
    "        }\n",
    "      </style>\n",
    "      <p class='instructions'>Review the image and choose all applicable categories.\n",
    "        If no categories apply, choose None.\n",
    "\n",
    "        <b>Nudity</b>\n",
    "        Visuals depicting nude male or female person or persons\n",
    "\n",
    "        <b>Graphic Male Nudity</b>\n",
    "        Visuals depicting full frontal male nudity, often close ups\n",
    "\n",
    "        <b>Graphic Female Nudity</b>\n",
    "        Visuals depicting full frontal female nudity, often close ups\n",
    "\n",
    "        <b>Sexual Activity</b>\n",
    "        Visuals depicting various types of explicit sexual activities and pornography\n",
    "\n",
    "        <b>Illustrated Nudity or Sexual Activity</b>\n",
    "        Visuals depicting animated or drawn sexual activity, nudity or pornography\n",
    "\n",
    "        <b>Adult Toys</b>\n",
    "        Visuals depicting adult toys, often in a marketing context\n",
    "\n",
    "        <b>Female Swimwear or Underwear</b>\n",
    "        Visuals depicting female person wearing only swimwear or underwear\n",
    "\n",
    "        <b>Male Swimwear Or Underwear</b>\n",
    "        Visuals depicting male person wearing only swimwear or underwear\n",
    "\n",
    "        <b>Partial Nudity</b>\n",
    "        Visuals depicting covered up nudity, for example using hands or pose\n",
    "\n",
    "        <b>Revealing Clothes</b>\n",
    "        Visuals depicting revealing clothes and poses, such as deep cut dresses\n",
    "\n",
    "        <b>Graphic Violence or Gore</b>\n",
    "        Visuals depicting prominent blood or bloody injuries\n",
    "\n",
    "        <b>Physical Violence</b>\n",
    "        Visuals depicting violent physical assault, such as kicking or punching\n",
    "\n",
    "        <b>Weapon Violence</b>\n",
    "        Visuals depicting violence using weapons like firearms or blades, such as shooting\n",
    "\n",
    "        <b>Weapons</b>\n",
    "        Visuals depicting weapons like firearms and blades\n",
    "\n",
    "        <b>Self Injury</b>\n",
    "        Visuals depicting self-inflicted cutting on the body, typically in distinctive patterns using sharp objects\n",
    "\n",
    "        <b>Emaciated Bodies</b>\n",
    "        Visuals depicting extremely malnourished human bodies\n",
    "\n",
    "        <b>Corpses</b>\n",
    "        Visuals depicting human dead bodies\n",
    "\n",
    "        <b>Hanging</b>\n",
    "        Visuals depicting death by hanging</p>\n",
    "    </short-instructions>\n",
    "\n",
    "    <full-instructions header=\"Instructions\"></full-instructions>\n",
    "  </crowd-rekognition-detect-moderation-labels>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-rekognition-demo-' + str(uuid.uuid4())\n",
    "\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Flow Definition\n",
    "\n",
    "Now we can create a Flow Definition. The Flow Definition encapsulates the following high-level concepts:\n",
    "\n",
    "* The AWS managed request source (such as AWS Rekognition content moderation **AWS/Rekognition/Image/ContentModeration/V3**)\n",
    "* Business Conditions against which ML inference is evaluated and a human loop is created if conditions are met\n",
    "* Workteam to review the human tasks, number of workers per task and other details\n",
    "* Task UI template\n",
    "* Output S3 location for the human task results\n",
    "\n",
    "Flow Definition is associated with a particular AWS managed request source, which affects the structure of the human loop activation conditions and format of the inference input and result.\n",
    "\n",
    "The human loop activation conditions used in this demo are tailored towards Amazon Rekognition content moderation - they are based on the confidence thresholds for particular moderation labels.\n",
    "\n",
    "Activation conditions can be expressed using logical operators *And*, *Or* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_definition():\n",
    "    '''\n",
    "    Creates a Flow Definition resource\n",
    "\n",
    "    Returns:\n",
    "    struct: FlowDefinitionArn\n",
    "    '''\n",
    "    humanLoopActivationConditions = json.dumps(\n",
    "        {\n",
    "            \"Conditions\": [\n",
    "                {\n",
    "                  \"Or\": [\n",
    "                    {\n",
    "                        \"ConditionType\": \"ModerationLabelConfidenceCheck\",\n",
    "                        \"ConditionParameters\": {\n",
    "                            \"ModerationLabelName\": \"Suggestive\",\n",
    "                            \"ConfidenceLessThan\": 98\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"ConditionType\": \"ModerationLabelConfidenceCheck\",\n",
    "                        \"ConditionParameters\": {\n",
    "                            \"ModerationLabelName\": \"Female Swimwear Or Underwear\",\n",
    "                            \"ConfidenceGreaterThan\": 98\n",
    "                        }\n",
    "                    }\n",
    "                  ]\n",
    "               }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = sagemaker.create_flow_definition(\n",
    "            FlowDefinitionName= flowDefinitionName,\n",
    "            RoleArn= ROLE,\n",
    "            HumanLoopConfig= {\n",
    "                \"WorkteamArn\": WORKTEAM_ARN,\n",
    "                \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "                \"TaskCount\": 1,\n",
    "                \"TaskDescription\": \"Demo A2I moderation sample task description\",\n",
    "                \"TaskTitle\": \"Demo A2I moderation sample task\"\n",
    "            },\n",
    "            HumanLoopRequestSource= {\n",
    "                \"AwsManagedHumanLoopRequestSource\": \"AWS/Rekognition/DetectModerationLabels/Image/V3\"\n",
    "            },\n",
    "            HumanLoopActivationConfig={\n",
    "                \"HumanLoopActivationConditionsConfig\": {\n",
    "                    \"HumanLoopActivationConditions\": humanLoopActivationConditions\n",
    "                }\n",
    "            },\n",
    "            OutputConfig={\n",
    "                \"S3OutputPath\" : OUTPUT_PATH\n",
    "            }\n",
    "        )\n",
    "    print_response(response)\n",
    "    return response['FlowDefinitionArn'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-rekognition-demo-' + str(uuid.uuid4())\n",
    "\n",
    "# Create a flow definition. We'll be using the flow definition arn for starting human loops.\n",
    "flow_definition_arn = create_flow_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_flow_definition(name):\n",
    "    '''\n",
    "    Describes Flow Definition\n",
    "\n",
    "    Returns:\n",
    "    struct: response from DescribeFlowDefinition API invocation\n",
    "    '''\n",
    "    return sagemaker.describe_flow_definition(\n",
    "        FlowDefinitionName=name)\n",
    "\n",
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = describe_flow_definition(flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Moderation Labels with AWS Rekognition\n",
    "\n",
    "Let's call Amazon Rekognition to detect moderation labels on the sample images stored in S3 based on the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueId = str(uuid.uuid4())\n",
    "\n",
    "def detect_moderation_labels(img_name):\n",
    "    response = rekognition.detect_moderation_labels(Image={'S3Object': {'Bucket': BUCKET, 'Name': img_name}}, \n",
    "                                                    HumanLoopConfig={'HumanLoopName': uniqueId + '-1', 'FlowDefinitionArn': flow_definition_arn})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: When Activation Conditions are met, a Human Loop is created\n",
    "\n",
    "When an image passed to Rekognition matches the conditions in FlowDefinition, a HumanLoopArn will be present in the response to detect_moderation_labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderationResponse1 = detect_moderation_labels(imageName1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HumanLoopArn' in moderationResponse1['HumanLoopActivationOutput']:\n",
    "    # A human loop has been started!\n",
    "    print(f'A human loop has been started: {moderationResponse1[\"HumanLoopActivationOutput\"][\"HumanLoopArn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_human_loops_in_workflow = a2i_runtime_client.list_human_loops(FlowDefinitionArn=flow_definition_arn)['HumanLoopSummaries']\n",
    "\n",
    "for human_loop in all_human_loops_in_workflow:\n",
    "    print(f'\\nHuman Loop Name: {human_loop[\"HumanLoopName\"]}')\n",
    "    print(f'Human Loop Status: {human_loop[\"HumanLoopStatus\"]} \\n')\n",
    "    print('\\n')\n",
    "    if human_loop['HumanLoopStatus'] == 'Completed':\n",
    "        completed_loops.append(human_loop['HumanLoopName'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_human_loops_in_workflow = a2i_runtime_client.list_human_loops(FlowDefinitionArn=flow_definition_arn)['HumanLoopSummaries']\n",
    "\n",
    "completed_loops = []\n",
    "for human_loop in all_human_loops_in_workflow:\n",
    "    print(f'\\nHuman Loop Name: {human_loop[\"HumanLoopName\"]}')\n",
    "    print(f'Human Loop Status: {human_loop[\"HumanLoopStatus\"]} \\n')\n",
    "    print('\\n')\n",
    "    if human_loop['HumanLoopStatus'] == 'Completed':\n",
    "        completed_loops.append(human_loop['HumanLoopName'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task Results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once work is completed, Amazon A2I stores results in your S3 bucket and sends a Cloudwatch event. Your results should be available in the S3 OUTPUT_PATH when all work is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def retrieve_a2i_results_from_output_s3_uri(bucket, a2i_s3_output_uri):\n",
    "    '''\n",
    "    Gets the json file published by A2I and returns a deserialized object\n",
    "    '''\n",
    "    splitted_string = re.split('s3://' +  bucket + '/', a2i_s3_output_uri)\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=bucket, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    return json.loads(content)\n",
    "    \n",
    "\n",
    "for human_loop_name in completed_loops:\n",
    "\n",
    "    describe_human_loop_response = a2i_runtime_client.describe_human_loop(\n",
    "        HumanLoopName=human_loop_name\n",
    "    )\n",
    "    \n",
    "    print(f'\\nHuman Loop Name: {describe_human_loop_response[\"HumanLoopName\"]}')\n",
    "    print(f'Human Loop Status: {describe_human_loop_response[\"HumanLoopStatus\"]}')\n",
    "    print(f'Human Loop Output Location: : {describe_human_loop_response[\"HumanLoopOutput\"][\"OutputS3Uri\"]} \\n')\n",
    "    \n",
    "    # Uncomment below line to print out a2i human answers\n",
    "    # pp.pprint(retrieve_a2i_results_from_output_s3_uri(BUCKET, describe_human_loop_response['HumanLoopOutput']['OutputS3Uri']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: When Activation Conditions are not met, Human Loop is not created\n",
    "\n",
    "Image passed to Rekognition does not match the conditions in FlowDefinition so HumanLoopArn will not be present in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderationResponse2 = detect_moderation_labels(imageName2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HumanLoopArn' in moderationResponse2['HumanLoopActivationOutput']:\n",
    "    # A human loop has been started!\n",
    "    print(f'A human loop has been started: {moderationResponse1[\"HumanLoopActivationOutput\"][\"HumanLoopArn\"]}')\n",
    "else:\n",
    "    print('No human loop started as none of the HumanLoopActivationConditions were met.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no **HumanLoopArn** is present, this implies that no HumanLoopActivationConditions were met and no Human Loop was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
